Train: 39832 sents / 2540 batches, Val: 1346 sents / 120 batches
Vocab size: 23773
--------------------------------------------------
Model Architecture:
TransformerGrammarPlusQNet(
  (tg_p_net): TransformerGrammar(
    (dropout): Dropout(p=0.4, inplace=False)
    (emb): Embedding(23773, 380)
    (projection): Linear(in_features=380, out_features=23773, bias=True)
    (layers): ModuleList(
      (0-11): 12 x TransformerGrammarLayer(
        (dec_attn): RelPartialLearnableMultiHeadAttn(
          (qkv_net): Sequential(
            (0): Linear(in_features=380, out_features=900, bias=False)
            (1): Dropout(p=0.2, inplace=False)
          )
          (drop): Dropout(p=0.2, inplace=False)
          (dropatt): Dropout(p=0.2, inplace=False)
          (o_net): Linear(in_features=300, out_features=380, bias=False)
          (layer_norm): LayerNorm((380,), eps=1e-05, elementwise_affine=True)
          (r_net): Linear(in_features=380, out_features=300, bias=False)
        )
        (pos_ff): PositionwiseFF(
          (CoreNet): Sequential(
            (0): Linear(in_features=380, out_features=900, bias=True)
            (1): ReLU(inplace=True)
            (2): Dropout(p=0.4, inplace=False)
            (3): Linear(in_features=900, out_features=380, bias=True)
            (4): Dropout(p=0.4, inplace=False)
          )
          (layer_norm): LayerNorm((380,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (pos_emb): PositionalEmbedding()
  )
  (q_crf): ConstituencyTreeCRF()
  (q_pos_emb): Embedding(250, 380)
  (q_leaf_rnn): LSTM(380, 300, batch_first=True, bidirectional=True)
  (q_binary): Sequential(
    (0): Linear(in_features=600, out_features=600, bias=True)
    (1): ReLU()
    (2): LayerNorm((600,), eps=1e-05, elementwise_affine=True)
    (3): Dropout(p=0.4, inplace=False)
    (4): Linear(in_features=600, out_features=1, bias=True)
  )
  (dropout): Dropout(p=0.4, inplace=False)
  (emb): Embedding(23773, 380)
)
--------------------------------------------------
Traceback (most recent call last):
  File "/public/home/zhouchy2022/urnng_framed_tg/tg_train.py", line 394, in <module>
    tg_main(args)
  File "/public/home/zhouchy2022/urnng_framed_tg/tg_train.py", line 209, in tg_main
    best_val_ll = tg_eval(val_data,
  File "/public/home/zhouchy2022/urnng_framed_tg/tg_train.py", line 369, in tg_eval
    log_likelihood, likeli_action_q_all, all_actions, q_entropy = model(
  File "/public/home/zhouchy2022/anaconda3/envs/urnng-plus-tg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/public/home/zhouchy2022/urnng_framed_tg/tg_model.py", line 859, in forward
    loss = self._forward_TG(input_batch=inputs,
  File "/public/home/zhouchy2022/urnng_framed_tg/tg_model.py", line 706, in _forward_TG
    return self.tg_p_net(input_batch, length, use_mask, document_level,
  File "/public/home/zhouchy2022/anaconda3/envs/urnng-plus-tg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/public/home/zhouchy2022/urnng_framed_tg/tg_model.py", line 529, in forward
    loss = loss.view(targets.size(0), targets.size(1)) # shape: (seq_len, batch_size)
RuntimeError: shape '[461, 3]' is invalid for input of size 1